---
title: "Assignment 7: High Frequency Data"
author: "Gabi Richichi"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## OVERVIEW

This exercise accompanies the lessons in Hydrologic Data Analysis on high frequency data

## Directions
1. Change "Student Name" on line 3 (above) with your name.
2. Work through the steps, **creating code and output** that fulfill each instruction.
3. Be sure to **answer the questions** in this assignment document.
4. When you have completed the assignment, **Knit** the text and code into a single pdf file.
5. After Knitting, submit the completed exercise (pdf file) to the dropbox in Sakai. Add your last name into the file name (e.g., "A07_Chamberlin.pdf") prior to submission.

The completed exercise is due on 16 October 2019 at 9:00 am.

## Setup

1. Verify your working directory is set to the R project file, 
2. Load the StreamPULSE, streamMetabolizer and tidyverse packages. 
3. Set your ggplot theme (can be theme_classic or something else)


```{r setup}
getwd()
library(StreamPULSE)
library(streamMetabolizer)
library(tidyverse)
theme_set(theme_classic())
```


4. Download data from the Stream Pulse portal using `request_data()` for the Kansas River, ("KS_KANSASR"). Download the discharge (`Discharge_m3s`), disolved oxygen (`DO_mgL`) and nitrate data (`Nitrate_mgL`) for the entire period of record

5. Reformat the data into one dataframe with columns DateTime_UTC, DateTime_Solar (using `convert_UTC_to_solartime()`), SiteName, DO_mgL, Discharge_m3s, and Nitrate_mgL.
```{r Datadownload}
citation('streamMetabolizer')

Data <- request_data(
  sitecode = "KS_KANSASR", 
  variables = c('Discharge_m3s', 'DO_mgL', 'Nitrate_mgL'))

Data.lon <- Data[[2]]$lon

Data.vars <- Data[[1]] %>%
  spread(value = value, key = variable) %>%
  mutate(DateTime_Solar = convert_UTC_to_solartime(DateTime_UTC, Data.lon))

Data.vars2 <- Data[[2]] %>%
  mutate(SiteName = name)

Data.combined <-
  left_join(Data.vars, Data.vars2) %>%
  select(DateTime_UTC, DateTime_Solar, SiteName, DO_mgL, Discharge_m3s, Nitrate_mgL)

```

6. Plot each of the 3 variables against solar time for the period of record

```{r}
DOPlot <-
  ggplot(Data.combined, aes(x = DateTime_Solar, y = DO_mgL)) + 
  geom_line() +
  ggtitle("Dissolved Oxygen Concentration vs. Solar Time") +
  labs(x = "Solar Time", y = "Dissolved Oxygen (mg/L)")
print(DOPlot)

DischargePlot <-
  ggplot(Data.combined, aes(x = DateTime_Solar, y = Discharge_m3s)) + 
  geom_line() +
  ggtitle("Discharge vs. Solar Time") +
  labs(x = "Solar Time", y = expression("Discharge (m"^3*"/s)"))
print(DischargePlot)

NitratePlot <-
  ggplot(Data.combined, aes(x = DateTime_Solar, y = Nitrate_mgL)) + 
  geom_line() +
  ggtitle("Nitrate Concentration vs. Solar Time") +
  labs(x = "Solar Time", y = "Nitrate (mg/L)")
print(NitratePlot)
```

7. How will you address gaps in these dataseries?

> I will address the gaps in these dataseries by filtering the series and dropping na values. This will be particularly helpful when conducting future analyses, such as the baseflow vs. quickflow partitioning calculations.

> Another solution is to simply select for analysis sections of the graphs that have consistent data and avoid sections with gaps. 

8. How does the daily amplitude of oxygen concentration swings change over the season? What might cause this?

> The daily amplitude of oxygen concentration swings gets larger as the season progresses from winter toward summer. As the temperature gets warmer, a greater quantity of photosynthetic action happens, and there is more production than usual. There is also a greater quantity of cellular respiration occurring as life grows and prospers in the warmer weather. As a result, the daily swings between high and low dissolved oxygen concentrations are greater, as there is a higher quantity of DO in the day (when both cellular respiration and photosynthesis occur) and a lower quantity of DO at night (when cellular respiration occurs but photosynthesis does not).

## Baseflow separation
9. Use the `EcoHydRology::BaseflowSeparation()` function to partition discharge into baseflow and quickflow, and calculate how much water was exported as baseflow and quickflow for this time period. Use the DateTime_UTC column as your timestamps in this analysis.

The `package::function()` notation being asked here is a way to call a function without loading the library. Sometimes the EcoHydRology package can mask tidyverse functions like pipes, which will cause problems for knitting. In your script, instead of just typing `BaseflowSeparation()`, you will need to include the package and two colons as well.

10. Create a ggplot showing total flow, baseflow, and quickflow together. 


```{r}
#install.packages("EcoHydRology")
library(EcoHydRology)

Data.dropna <- drop_na(Data.combined)

BaseFlow <- BaseflowSeparation(
  Data.dropna$Discharge_m3s, 
  filter_parameter = 0.925 
  #passes = 3,
  )

FlowData <- cbind(Data.dropna, BaseFlow)

FlowPlot <- ggplot(FlowData, aes(x = DateTime_UTC, y = Discharge_m3s)) + 
  geom_line() +
  # scale_y_log10() +
  geom_line(mapping = aes(x = DateTime_UTC, y = bt), color = "darkorange4") +
  geom_line(mapping = aes(x = DateTime_UTC, y = qft), color = "steelblue4") +
  labs(x = "DateTime", y = expression("Discharge (m"^3*"/s)")) +
  ggtitle("Discharge vs. Time")
print(FlowPlot)

#DataWithDecimalDate <- FlowData %>%
#  mutate(FlowData, DateAsDecimal = convert_date_to_doyhr(c(1, as.POSIXlt(DateTime_UTC))))
#print(length(c(zero,diff(as.numeric(FlowData$DateTime_UTC),1,1))))

print(c(0, diff(as.numeric(FlowData$DateTime_UTC))))


Export <-  mutate(FlowData, timestep = c(0, diff(as.numeric(FlowData$DateTime_UTC))),
       baseflowexport = bt * timestep,
       quickflowexport = qft * timestep) 
print(FlowData$quickflowexport)
Summary <-  
  summarize(Export, BaseflowExport_cf = sum(baseflowexport),
  QuickflowExport_cf = sum(quickflowexport),
  TotalExport_cf = BaseflowExport_cf + QuickflowExport_cf)

PercentQF <- Summary$QuickflowExport_cf/Summary$TotalExport_cf*100
PercentBF <- Summary$BaseflowExport_cf/Summary$TotalExport_cf*100
```


11. What percentage of total water exported left as baseflow and quickflow from the Kansas River over this time period?

> A much greater percentage of total water exported left as baseflow than as quickflow. Specifically, 96% of water exported left as baseflow, and 4% of water exported left as quickflow. 

12. This is a much larger river and watershed than the 2 we investigated in class. How does the size of the watershed impact how flow is partitioned into quickflow and baseflow? 

> The larger the watershed, the more flow will be partitioned into baseflow, because a larger watershed has a greater volume capacity to store water between precipitation events. 

13. The site we are looking at is also further down in its river network (i.e. instead of being a headwater stream, this river has multiple tributaries that flow into it). How does this impact your interpretation of your results?

> The further down a river network a site is, the more flow will be partitioned into baseflow. This is because there is less influence by quickflow from precipitation events further down the network. 

## Chemical Hysteresis

14. Create a ggplot of flow vs. nitrate for the large storm in May (~May 1 - May 20). Use color to represent Date and Time.

```{r}
library(lubridate)
DataWithMonth <- 
  mutate(FlowData, Month = month(FlowData$DateTime_UTC), 
         Day = day(FlowData$DateTime_UTC))

StormData <- 
  filter(DataWithMonth, Month == 5 & Day <=20)

NitrateFlowPlot <- ggplot(StormData, aes(x = Nitrate_mgL, y = Discharge_m3s, color = DateTime_UTC)) +
  geom_point()
print(NitrateFlowPlot)

```

15. Does this storm show clockwise or counterclockwise hysteresis? Was this storm a flushing or diluting storm?

> This storm shows clockwise hysteresis. This storm was a flushing storm. 

16. What does this mean for how nitrate gets into the river from the watershed?

> Nitrate enters the river through the rising portion of the storm, as discharge increases, and then leaves during the falling portion of the storm, when discharge decreases.

## Reflection
17. What are 2-3 conclusions or summary points about high frequency data you learned through your analysis?

> 1. High frequency data is useful in examining storm events. You can process data values for variables such as discharge and conductance or nitrate levels and determine whether the storm induced a flushing or diluting event.

> 2. High frequency data can elucidate patterns over the course of a year, as well as over the course of a week. For example, you can discern tendencies in dissolved oxygen concentrations for winter months versus summer months. In this case of this assignment, dissolved oxygen concentrations had larger ranges for summer months than winter months.

> 3. Many times high frequency data has gaps due to lack of sampling during periods of time. During experimental data wrangling, sections of time with consistent data can be selected in order to conduct proper analyses. 

18. What data, visualizations, and/or models supported your conclusions from 17?

> The hysteresis plot helped me visualize the concept of flushing and diluting events. Additionally, the DO vs. solar time plot helped me visualize patterns in DO over the course of a year.

19. Did hands-on data analysis impact your learning about high frequency data relative to a theory-based lesson? If so, how?

> Yes, I definitely believe I have committed to memory the concepts and the coding procedures of analyzing high frequency data due to this learning process.

20.	How did the real-world data compare with your expectations from theory?

> I expected storm events to have an effect on nutrient levels. When Hurricane Florence passed through North Carolina, I remember reading in the news about discharge from the storm transporting hog waste and coal ash into waterbodies. This is in alignment with the concepts of nutrient loading due to storm events. 
